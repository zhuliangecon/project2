\documentclass[12pt,a4paper]{article}

% load packages
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{parskip}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}

% set code style
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% set lst style
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   % 设置背景颜色
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,    % 使用等宽字体
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}


\title{Project 2 Report for Probelm 2.1}
\author{Zhu Liang}
\date{\today}

\begin{document}

\maketitle

\section{Project Description}
In this project, we undertake two primary tasks. 
The first task is to provide a detailed description of the algorithms behind five collective communication operations provided by MPI.
The description are provided in Section \ref{sec:mpi_buildin}.
The second task is an empirical comparison between the built-in \texttt{MPI\_Bcast} and a custom broadcast implementation named \texttt{MY\_Bcast()}.


\section{Algorithm Description}
\subsection{MPI Build-in Operations}
\label{sec:mpi_buildin}


\textbf{\texttt{MPI\_Bcast()}}: This function broadcasts a message from the process with the designated root rank to all other processes in the communicator. All processes must call this function, with matching arguments.

\textbf{\texttt{MPI\_Scatter()}}: This function distributes distinct blocks of data from the root process to each process in the communicator. The root sends data to itself as well as to the other processes.

\textbf{\texttt{MPI\_Allgather()}}: Each process sends its own data to all other processes and gathers data from all processes. At the end, every process has the data from all the other processes.

\textbf{\texttt{MPI\_Alltoall()}}: This function allows each process to send distinct data to every other process. It generalizes the functionality of both scatter and gather, with different data being sent to each process.

\textbf{\texttt{MPI\_Reduce()}}: All processes in the communicator contribute their own data, which is combined (reduced) into a single result using a specified operation, like sum, max, etc. The result is stored on the root process.


\subsection{Custom Broadcast Function: \texttt{MY\_Bcast()}}
The \texttt{MY\_Bcast()} function is designed to mimic the MPI broadcast functionality using non-blocking send and receive  calls. 
It works by designating one process as the root, which sends the content of its buffer to all other processes. 
Each of the other processes waits to receive this content and store it in its own buffer.

For more details on the project structure and the implementation, please refer to the \texttt{README.md} file in the \texttt{project2} folder.

The source code of the \texttt{MY\_Bcast()} function is included in \texttt{functions.c} file. 
The pseudocode is shown below.
\begin{algorithm}
    \caption{Custom Broadcast Function}
    \begin{algorithmic}[1]
    \Procedure{MY\_ Bcast}{}
    \State \textbf{Get} rank and size of processes
        \If{current process rank is \textit{root}}
            \For{all the other processes \textit{i}}
                \State \textbf{Send} content of buffer to process \textit{i}
            \EndFor
        \Else
            \State \textbf{Receive} content from \textit{root} process into buffer
        \EndIf
        
        \State \Return MPI\_SUCCESS
    \EndProcedure
    \end{algorithmic}
\end{algorithm}


\section{Results}

In the following tables, 
we present the execution times for two different broadcast implementations: 
\texttt{MPI\_Bcast} and \texttt{MY\_Bcast}. 
It is essential to note that while individual run times might vary depending on the specific runtime environment, 
the general pattern observed should remain similar across different runs.

\begin{table}[!htb]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    & P = 4 & P = 7 & P = 28 & P = 37 \\
    \hline
    $N = 2^{10}$ & 0.000016s & 0.000015s & 0.000057s & 0.000092s \\
    \hline
    $N = 2^{12}$ & 0.000013s & 0.000023s & 0.000076s & 0.000089s \\
    \hline
    $N = 2^{14}$ & 0.000029s & 0.000067s & 0.000128s & 0.000241s \\
    \hline
    $N = 2^{16}$ & 0.000095s & 0.000164s & 0.000356s & 0.000655s \\
    \hline
    \end{tabular}
    \caption{Execution time using \texttt{MPI\_Bcast}}
\end{table}

\begin{table}[!htb]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    & P = 4 & P = 7 & P = 28 & P = 37 \\
    \hline
    $N = 2^{10}$ & 0.000014s & 0.000026s & 0.000158s & 0.000218s \\
    \hline
    $N = 2^{12}$ & 0.000015s & 0.000032s & 0.000194s & 0.000293s \\
    \hline
    $N = 2^{14}$ & 0.000031s & 0.000060s & 0.000357s & 0.000582s \\
    \hline
    $N = 2^{16}$ & 0.000086s & 0.000175s & 0.001076s & 0.001689s \\
    \hline
    \end{tabular}
    \caption{Execution time using \texttt{MY\_Bcast}}
\end{table}

\section{Analysis}

In this section, we delve into the observed performance variations between the two broadcasting functions, \texttt{MPI\_Bcast} and \texttt{MY\_Bcast}. Our primary objective is to decipher the underlying reasons behind their distinct behaviors under varying conditions.

\subsection{Summary of Results}
From the results presented in the previous section, it is evident that \texttt{MY\_Bcast} tends to perform better with a smaller number of cores, while \texttt{MPI\_Bcast} shows superior performance as the number of cores increases.

\subsection{Possible Explanations}
Several hypotheses might explain the observed performance trends:

\begin{enumerate}
    \item \textbf{Simplicity of Implementation:} The \texttt{MY\_Bcast} is simpler, without involving hierarchical broadcasting, considerations for network topology, or other advanced features. In situations with fewer cores, this simplicity might lead to higher efficiency.
    
    \item \textbf{Startup Overhead:} The \texttt{MPI\_Bcast} operation might have certain startup overheads, especially when setting up the broadcast operation or initializing network communications. As the number of cores increases, these overheads get amortized over more parallel work, becoming a smaller concern. However, with fewer cores, these overheads might be more pronounced.
    
    \item \textbf{Advanced Optimizations and Features:} \texttt{MPI\_Bcast} might employ various advanced optimizations and features. These optimizations might become evident in large-scale parallel environments but might not be as prominent with fewer core counts.
\end{enumerate}


\subsection{Conclusion}
The comparison offers valuable insights into parallel computing. 
The key takeaway is the importance of understanding the relationship between software design and hardware. 
It's evident that the right choice depends on the specific scenario and that no single solution is universally optimal. 

\section*{File Notes}
The source code of the program is in the \texttt{project2} folder. 
For more details, please refer to the \texttt{README.md} file in the folder.

\end{document}
